#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
# Copyright (C) 2012 Yamauchi, Hitoshi
# For Rebecca from Hitoshi the fool
#
# Scan directory and generate the vector. Use blacklist.
#

import os, sys
import codecs

class FileVectorExtractor(object):
    """create a vector from scanning a directory with blacklist.
    Because Wiki uses link destination as file names, scanning
    diretory can generate the vector."""

    def __init__(self):
        """constructor
        """
        sys.stdout = codecs.getwriter('utf_8')(sys.stdout)
        self.__black_list_set = set()


    # def can_add_entry(self, _link):
    #     """check the _link has real link in Wiki
    #     \param[in] _link a link entry, i.e. 'a' entry
    #     \return True when a link exists
    #     """
    #     # filter out a specific class
    #     linkclass = _link.get('class')
    #     if ((linkclass != None) and (len(linkclass) > 0) and \
    #             (linkclass[0] in self.__ignore_link_class_set)):
    #         return False

    #     # no title means no link
    #     linktitle = _link.get('title')
    #     if (linktitle == None):
    #         return False

    #     # filter out some specific links in Wikipedia
    #     href_string = _link.get('href')
    #     for words in self.__ignore_href_substring_list:
    #         if ( href_string.find(words) >= 0):
    #             return False
    #     return True


    # def get_link_list(self, _root_url, _output_fname):
    #     """get link list
    #     \param[in] _root_url     root file URL
    #     \param[in] _output_fname output list filename
    #     """
    #     try:
    #         # open output file with utf-8 codec
    #         outfile = codecs.open(_output_fname, mode='w', encoding='utf-8')

    #         outfile.write(u'#FileVectorExtractor 0\n');
    #         outfile.write(u'# generated by FileVectorExtractor. (C) Hitoshi 2012\n');
    #         outfile.write(u'# input [' + _root_url     + ']\n');

    #         data = urllib2.urlopen(_root_url).read()
    #         soup = BeautifulSoup(data)
    #         linklist = soup.find_all('a')
    #         for link in linklist:
    #             # print link.get('class')
    #             if self.can_add_entry(link) == False:
    #                 # print 'ignore: ' + str(link)
    #                 pass
    #             else:
    #                 # In Wiki, href and title are the same. No need to
    #                 # output both.
    #                 #
    #                 # outfile.write(link.get('href') + ' ' + link.get('title') + '\n');
    #                 outfile.write(link.get('href') + '\n');

    #         outfile.close()
    #     except IOError as (errno, strerror):
    #         print "# I/O error({0}): {1}".format(errno, strerror)
    #         print '# Does output directory exist?'
    #         print '# Also you need LC_ALL setting to utf-8, e.g., en_US.utf-8, ja_JP.utf-8.'

    #     # Uncaught exception goes up.

    def read_blacklist(self, _blacklist_fname):
        # NIN
        pass


    def get_vector(self, _input_dir, _output_file):
        """get vector from scanning directory.
        \param[in] _input_dir    input files' directory
        \param[in] _output_file  output file name
        """
        if (not os.path.exists(_input_dir)):
            raise StandardError, ('No such input directory [' + _input_dir + ']')
        if (os.path.exists(_output_file)):
            raise StandardError, ('Output file exists [' + _output_file + ']')

        os.chdir(_input_dir)
        for fname in os.listdir("."):
            if (not os.path.isfile(fname)):
                continue
            ufname = unicode(fname, encoding='utf-8', errors='strict')
            # outfpath = os.path.join(_output_dir, ufname)
            # self.fix_one_file(ufname, outfpath)
            print ufname


if __name__=="__main__":

    print u'# Usage: run the test_filevectorextractor_0.py.'
